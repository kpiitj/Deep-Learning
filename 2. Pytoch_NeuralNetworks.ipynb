{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXnnnWj98q2L"
      },
      "source": [
        "## Neural Networks\n",
        "\n",
        "We can construct neural networks using the <mark style=\"background-color: Yellow\">torch.nn</mark> package.\n",
        "\n",
        "Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input)that returns the output.\n",
        "\n",
        "\n",
        "### convnet\n",
        "\n",
        "It is a simple feed-forward network. It takes the input,feeds it through several layers one after the other, and then finally gives the output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "- Define the neural network that has some learnable parameters (or weights)\n",
        "- Iterate over a dataset of inputs\n",
        "- Process input through the network\n",
        "- Compute the loss (how far is the output from being correct)\n",
        "- Propagate gradients back into the network’s parameters\n",
        "- Update the weights of the network, typically using a simple update rule: <mark style=\"background-color: light-blue\">weight = weight - learning_rate * gradient</mark>\n",
        "        \n",
        "### Define the network\n",
        "\n",
        "Let's define the network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umOuGWL_8q2L",
        "outputId": "2f5ac45a-273d-49fa-8e08-6726f28782bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "network(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class network(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(network, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = mx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = network()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qegjtzQ98q2L"
      },
      "source": [
        "You just have to define the <mark style=\"background-color: yellow\">forward</mark>,  and the <mark style=\"background-color: yellow\">backward</mark> function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the <mark style=\"background-color: yellow\">forward</mark> function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Practice Questions\n",
        "1. Try printing the number of parameters in each layer."
      ],
      "metadata": {
        "id": "6Y-acXe5DbAI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2QvQAncNDy9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Modify the network to include dropout layers. How does it affect the output?"
      ],
      "metadata": {
        "id": "Zf0QSP7OD2KD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ny7idB7WD23j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement a function to calculate the total number of trainable parameters."
      ],
      "metadata": {
        "id": "EmPafWSlD512"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GL-B3_wED6qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting learnable parameters\n",
        "The learnable parameters of a model are returned by <mark style=\"background-color: yellow\">net.parameters()</mark>."
      ],
      "metadata": {
        "id": "6MP4xlfqmimF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVfi3CHE8q2M",
        "outputId": "f829a6a9-d86a-4307-cee5-69e485ed7d62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSJ7GWw8q2M"
      },
      "source": [
        "Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MSF0HId8q2M",
        "outputId": "488b485e-2cdd-4ef9-b439-e1f8b820c341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0426,  0.1086,  0.0513,  0.0206, -0.0837,  0.0527, -0.1580, -0.0314,\n",
            "          0.0627, -0.1058]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "inp = torch.randn(1, 1, 32, 32)\n",
        "out = net(inp)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Practice\n",
        "1. Modify the input to have a batch size of 5 and check if the model runs correctly."
      ],
      "metadata": {
        "id": "oCiLyXIrFL8z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ypqkv6gDFNGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Change the input size to 64x64 and modify the network accordingly."
      ],
      "metadata": {
        "id": "I2gvJ_eXFSTt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DsvrjUitFVd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implement batch normalization after each linear layer."
      ],
      "metadata": {
        "id": "CjAEOqAwFWRl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vN-wJEAxFY6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv5kzpRv8q2M"
      },
      "source": [
        "#### Computing Zero grad\n",
        "Zero the gradient buffers of all parameters and backprops with random gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzyJw6RZ8q2N"
      },
      "outputs": [],
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kj2QHLy8q2N"
      },
      "source": [
        "__Note:__\n",
        "<mark style=\"background-color: yellow\">torch.nn</mark> only supports mini-batches. The entire <mark style=\"background-color: yellow\">torch.nn</mark> package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
        "For example, <mark style=\"background-color: yellow\">nn.Conv2d</mark> will take in a 4D Tensor of <mark style=\"background-color: yellow\">nSamples x nChannels x Height x Width</mark>.\n",
        "If you have a single sample, just use <mark style=\"background-color: yellow\">input.unsqueeze(0)</mark> to add a fake batch dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYD_dc_M8q2N"
      },
      "source": [
        "__At this point, we covered:__\n",
        "- Defined neutral network\n",
        "- Processing input and calling backward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX5uhk1_8q2N"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
        "There are several different loss functions under the nn package . A simple loss is: <mark style=\"background-color: yellow\">nn.MSELoss</mark> which computes the mean-squared error between the input and the target.\n",
        "    \n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlZlX7Tx8q2N",
        "outputId": "62d1adf1-ade6-4e6f-9fd2-a788d12f3958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.5992, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = net(inp)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Practice\n",
        "1. Change the loss function to CrossEntropyLoss and modify the target accordingly."
      ],
      "metadata": {
        "id": "eNwwVlxbFmU8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zSArDNBzFscY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement a function to log the loss value at each epoch."
      ],
      "metadata": {
        "id": "Ga7ZfsyIFu2H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TeK3MLxqFvyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compare different loss functions and analyze their impact on training."
      ],
      "metadata": {
        "id": "umoC3jqpFyQg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24KE9OoUFzAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbEGZ-28q2N"
      },
      "source": [
        "Now, if you follow loss in the backward direction, using its <mark style=\"background-color: yellow\">.grad_fn</mark> attribute, you will see a graph of computations that looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAMDCJbw8q2O"
      },
      "source": [
        "input  ->   conv2d  ->   relu  ->   maxpool2d  ->   conv2d ->   relu  ->   maxpool2d\n",
        "      \n",
        "      -> view -> linear -> relu -> linear -> relu -> linear\n",
        "      \n",
        "      -> MSELoss\n",
        "      \n",
        "      -> loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HkeiSSe8q2O"
      },
      "source": [
        "So, when we call <mark style=\"background-color: yellow\">loss.backward()</mark>, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has requires_grad=True will have their .grad Tensor accumulated with the gradient.\n",
        "\n",
        "\n",
        "For illustration, let us follow a few steps <mark style=\"background-color: yellow\">backward</mark>:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMUNLYF-8q2O",
        "outputId": "6dff2ef2-02d7-45df-98dd-dc2fe9cb4498",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MseLossBackward0 object at 0x784a7a4f0a90>\n",
            "<AddmmBackward0 object at 0x784a7a4f3a90>\n",
            "<AccumulateGrad object at 0x784a7a4f0a90>\n"
          ]
        }
      ],
      "source": [
        "print(loss.grad_fn)  # MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABrQ_TSt8q2P"
      },
      "source": [
        "### Backprop\n",
        "\n",
        "To backpropagate the error all we have to do is to <mark style=\"background-color: yellow\">loss.backward()</mark>. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
        "\n",
        "\n",
        "Now we shall call <mark style=\"background-color: yellow\">loss.backward()</mark>, and have a look at conv1’s bias gradients before and after the backward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrF9mIK18q2Q",
        "outputId": "9228927a-3ba4-491d-d490-7a11c20b54ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "None\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0039, -0.0066,  0.0013,  0.0068, -0.0038, -0.0091])\n"
          ]
        }
      ],
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practice Question\n",
        "1. Implement a function to visualize gradient flow in the model."
      ],
      "metadata": {
        "id": "tIxCSOHDG89X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ZvVrHMyHDyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Modify the loss.backward() call to accumulate gradients over multiple batches."
      ],
      "metadata": {
        "id": "kGp00fE8HEQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kcfz9QWgHHOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Check if using loss.backward(retain_graph=True) affects training."
      ],
      "metadata": {
        "id": "GNt2rxaVHJzc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFJ0NqykHKhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfmU8LTS8q2Q"
      },
      "source": [
        "### Update the weights\n",
        "\n",
        "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
        "    \n",
        "   <mark style=\"background-color: yellow\"> weight = weight - learning_rate * gradient </mark>\n",
        "   \n",
        "We can implement this using simple Python code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0jqFf0c8q2Q"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Practice Questions\n",
        "1. Modify the learning rate and observe its impact on convergence."
      ],
      "metadata": {
        "id": "-1b4U9XWHNat"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UF4a8GLHHSnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement an adaptive learning rate that decreases over epochs."
      ],
      "metadata": {
        "id": "E7E8eW1IHTR3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEaxdjI6HVx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compare different optimizers like Adam and RMSprop on the same model."
      ],
      "metadata": {
        "id": "SJiBy5rpHWa1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hu02pIDGHYts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLeG-Xdw8q2Q"
      },
      "source": [
        "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwFBstG18q2Q"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(inp)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghYz7RJ8q2R"
      },
      "source": [
        "Observe how gradient buffers had to be manually set to zero using <mark style=\"background-color:yellow\">optimizer.zero_grad()</mark>. This is because gradients are accumulated as explained in the <mark style=\"font-color:red\">Backprop</mark> section."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Neural Network on Dummy MNIST\n",
        "\n",
        "-  Here we demonstrate training a simple feedforward network on a subset of the MNIST dataset."
      ],
      "metadata": {
        "id": "2_4i9f-jb_w6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# Define transformations: convert images to tensors and normalize.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Download the MNIST training dataset.\n",
        "mnist_full = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "# For quick demonstration, we use a subset (e.g., first 1000 samples).\n",
        "subset_indices = list(range(1000))\n",
        "mnist_subset = Subset(mnist_full, subset_indices)\n",
        "# Create a DataLoader for batching.\n",
        "train_loader = DataLoader(mnist_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define a simple feedforward network (MLP) for MNIST classification.\n",
        "class SimpleMNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMNISTNet, self).__init__()\n",
        "        # Input size: 28*28 = 784. Two hidden layers, and 10 outputs for the digits.\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the image tensor.\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network, define loss and optimizer.\n",
        "mnist_model = SimpleMNISTNet()\n",
        "criterion_mnist = nn.CrossEntropyLoss()\n",
        "optimizer_mnist = optim.SGD(mnist_model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop for a few epochs (for demonstration purposes, we use 2 epochs).\n",
        "num_epochs = 2\n",
        "print(\"\\nStarting training on MNIST subset...\")\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer_mnist.zero_grad()   # Clear gradients.\n",
        "        outputs = mnist_model(data)     # Forward pass.\n",
        "        loss = criterion_mnist(outputs, target)  # Compute loss.\n",
        "        loss.backward()                 # Backward pass.\n",
        "        optimizer_mnist.step()          # Update weights.\n",
        "        running_loss += loss.item()\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            avg_loss = running_loss / 10\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}], Loss: {avg_loss:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"MNIST training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH0Wo3Yhb40D",
        "outputId": "ac3f21ed-d3c5-45db-aa25-33297d1fbc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training on MNIST subset...\n",
            "Epoch [1/2], Batch [10], Loss: 2.3012\n",
            "Epoch [2/2], Batch [10], Loss: 2.2612\n",
            "MNIST training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice Questions\n",
        "1. Modify the batch size and analyze its impact on training."
      ],
      "metadata": {
        "id": "GKQaFLnMHkRj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLpMWdMTHqd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Implement early stopping in the training loop."
      ],
      "metadata": {
        "id": "oFLDT8uyHsUw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pFErbR8aHs-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Use a validation split and track validation accuracy after each epoch."
      ],
      "metadata": {
        "id": "S1cZoOflHu5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2ibspgcHvhM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}